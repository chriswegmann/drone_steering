{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Develop RNN Models for Human Activity Recognition Time Series Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on Jason Brownlee's MachineLearningMastery article:\n",
    "https://machinelearningmastery.com/how-to-develop-rnn-models-for-human-activity-recognition-time-series-classification/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UCI HAR Dataset can be found in our google drive: https://drive.google.com/open?id=1leUN60nh7FJGdtMrUpjSBvUS34OkUSZy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from keras.utils import to_categorical\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put the Dataset in a Data folder, which should be included in your .gitignore!!!\n",
    "I used drone_steering/Tutorial_DataSets/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../../Tutorial_DataSets/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Labels\n",
    "1) Walking\n",
    "<br>2) Walking Upstairs\n",
    "<br>3) Walking Downstairs\n",
    "<br>4) Sitting\n",
    "<br>5) Standing\n",
    "<br>6) Laying"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Investigate Basic File Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['body_acc_x_train.txt',\n",
       " 'body_acc_y_train.txt',\n",
       " 'body_acc_z_train.txt',\n",
       " 'body_gyro_x_train.txt',\n",
       " 'body_gyro_y_train.txt',\n",
       " 'body_gyro_z_train.txt',\n",
       " 'total_acc_x_train.txt',\n",
       " 'total_acc_y_train.txt',\n",
       " 'total_acc_z_train.txt']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir(data_dir + 'HARDataset/UCI HAR Dataset/train/Inertial Signals/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7352, 128)\n",
      "(7352, 128)\n"
     ]
    }
   ],
   "source": [
    "# inspect the data files (there are 9 files in total, one for each feature; here investigate only 2 of them)\n",
    "total_acc_x_train = pd.read_csv(\n",
    "    data_dir + 'HARDataset/UCI HAR Dataset/train/Inertial Signals/total_acc_x_train.txt', \n",
    "    header = None,\n",
    "    delim_whitespace = True\n",
    ")\n",
    "total_acc_y_train = pd.read_csv(\n",
    "    data_dir + 'HARDataset/UCI HAR Dataset/train/Inertial Signals/total_acc_y_train.txt', \n",
    "    header = None,\n",
    "    delim_whitespace = True\n",
    ")\n",
    "print(total_acc_x_train.shape)\n",
    "print(total_acc_y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7352, 128, 2)\n"
     ]
    }
   ],
   "source": [
    "stackem = np.dstack([total_acc_x_train,total_acc_y_train])\n",
    "print(stackem.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a single file as a numpy array\n",
    "def load_file(filepath):\n",
    "    dataframe = pd.read_csv(filepath, header=None, delim_whitespace=True)\n",
    "    return dataframe.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a list of files and return as a 3d numpy array\n",
    "def load_group(filenames, prefix=''):\n",
    "    \n",
    "    loaded = list()\n",
    "    \n",
    "    for name in filenames:\n",
    "        data = load_file(prefix + name)\n",
    "        loaded.append(data)\n",
    "    \n",
    "    # stack group so that features are the 3rd dimension\n",
    "    loaded = np.dstack(loaded)\n",
    "    \n",
    "    return loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a dataset group, such as train or test\n",
    "def load_dataset_group(group, prefix=''):\n",
    "    filepath = prefix + group + '/Inertial Signals/'\n",
    "    \n",
    "    # load all 9 files as a single array\n",
    "    filenames = list()\n",
    "    \n",
    "    # total acceleration\n",
    "    filenames += ['total_acc_x_'+group+'.txt', 'total_acc_y_'+group+'.txt', 'total_acc_z_'+group+'.txt']\n",
    "    \n",
    "    # body acceleration\n",
    "    filenames += ['body_acc_x_'+group+'.txt', 'body_acc_y_'+group+'.txt', 'body_acc_z_'+group+'.txt']\n",
    "    \n",
    "    # body gyroscope\n",
    "    filenames += ['body_gyro_x_'+group+'.txt', 'body_gyro_y_'+group+'.txt', 'body_gyro_z_'+group+'.txt']\n",
    "    \n",
    "    # load input data\n",
    "    X = load_group(filenames, filepath)\n",
    "    \n",
    "    # load class output\n",
    "    y = load_file(prefix + group + '/y_'+group+'.txt')\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset, returns train and test X and y elements\n",
    "def load_dataset(prefix=data_dir):\n",
    "    \n",
    "    # load all train\n",
    "    trainX, trainy = load_dataset_group('train', prefix + 'HARDataset/UCI HAR Dataset/')\n",
    "    print(trainX.shape, trainy.shape)\n",
    "    \n",
    "    # load all test\n",
    "    testX, testy = load_dataset_group('test', prefix + 'HARDataset/UCI HAR Dataset/')\n",
    "    print(testX.shape, testy.shape)\n",
    "    \n",
    "    # zero-offset class values\n",
    "    trainy = trainy - 1\n",
    "    testy = testy - 1\n",
    "    \n",
    "    # one hot encode y\n",
    "    trainy = to_categorical(trainy)\n",
    "    testy = to_categorical(testy)\n",
    "    print(trainX.shape, trainy.shape, testX.shape, testy.shape)\n",
    "    \n",
    "    return trainX, trainy, testX, testy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7352, 128, 9) (7352, 1)\n",
      "(2947, 128, 9) (2947, 1)\n",
      "(7352, 128, 9) (7352, 6) (2947, 128, 9) (2947, 6)\n"
     ]
    }
   ],
   "source": [
    "# each of the 9 features was splitted into fixed windows of 2.56 seconds (128 data points), with 50% overlap\n",
    "trainX, trainy, testX, testy = load_dataset(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# overlap\n",
    "trainX[0,64:,0] == trainX[1,:64,0] # 1st feature, 1st & 2nd sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Fitting & Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit and evaluate a model\n",
    "def evaluate_model(trainX, trainy, testX, testy):\n",
    "    \n",
    "    verbose, epochs, batch_size = 1, 15, 64\n",
    "    n_timesteps = trainX.shape[1] # here 128\n",
    "    n_features = trainX.shape[2] # here 9\n",
    "    n_outputs =  trainy.shape[1] # here 6 (number of labels)\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(LSTM(100, input_shape=(n_timesteps,n_features)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(n_outputs, activation='softmax'))\n",
    "    \n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy', \n",
    "        optimizer='adam', \n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # fit network\n",
    "    model.fit(\n",
    "        trainX, \n",
    "        trainy, \n",
    "        epochs=epochs, \n",
    "        batch_size=batch_size, \n",
    "        verbose=verbose\n",
    "    )\n",
    "    \n",
    "    # evaluate model\n",
    "    _, accuracy = model.evaluate(testX, testy, batch_size=batch_size, verbose=0)\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_7 (LSTM)                (None, 100)               44000     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 6)                 606       \n",
      "=================================================================\n",
      "Total params: 54,706\n",
      "Trainable params: 54,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "verbose, epochs, batch_size = 0, 15, 64\n",
    "\n",
    "n_timesteps = trainX.shape[1] # here 128\n",
    "n_features = trainX.shape[2] # here 9\n",
    "n_outputs =  trainy.shape[1] # here 6 (number of labels)\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(n_timesteps,n_features)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(n_outputs, activation='softmax'))\n",
    "\n",
    "model.compile(\n",
    "        loss='categorical_crossentropy', \n",
    "        optimizer='adam', \n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "7352/7352 [==============================] - 8s 1ms/step - loss: 1.2363 - acc: 0.4808\n",
      "Epoch 2/15\n",
      "7352/7352 [==============================] - 7s 986us/step - loss: 0.7773 - acc: 0.6650\n",
      "Epoch 3/15\n",
      "7352/7352 [==============================] - 7s 985us/step - loss: 0.5796 - acc: 0.7641\n",
      "Epoch 4/15\n",
      "7352/7352 [==============================] - 8s 1ms/step - loss: 0.4128 - acc: 0.8444\n",
      "Epoch 5/15\n",
      "7352/7352 [==============================] - 8s 1ms/step - loss: 0.3830 - acc: 0.8671\n",
      "Epoch 6/15\n",
      "7352/7352 [==============================] - 8s 1ms/step - loss: 0.2841 - acc: 0.9003\n",
      "Epoch 7/15\n",
      "7352/7352 [==============================] - 7s 974us/step - loss: 0.1931 - acc: 0.9241\n",
      "Epoch 8/15\n",
      "7352/7352 [==============================] - 7s 984us/step - loss: 0.1733 - acc: 0.9346\n",
      "Epoch 9/15\n",
      "7352/7352 [==============================] - 7s 958us/step - loss: 0.1694 - acc: 0.9348\n",
      "Epoch 10/15\n",
      "7352/7352 [==============================] - 7s 978us/step - loss: 0.2093 - acc: 0.9242\n",
      "Epoch 11/15\n",
      "7352/7352 [==============================] - 7s 977us/step - loss: 0.1389 - acc: 0.9461\n",
      "Epoch 12/15\n",
      "7352/7352 [==============================] - 7s 958us/step - loss: 0.1572 - acc: 0.9407\n",
      "Epoch 13/15\n",
      "7352/7352 [==============================] - 7s 938us/step - loss: 0.1276 - acc: 0.9474\n",
      "Epoch 14/15\n",
      "7352/7352 [==============================] - 7s 945us/step - loss: 0.1233 - acc: 0.9486\n",
      "Epoch 15/15\n",
      "7352/7352 [==============================] - 7s 950us/step - loss: 0.1210 - acc: 0.9487\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f662365ef0>"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "        trainX, \n",
    "        trainy, \n",
    "        epochs=epochs, \n",
    "        batch_size=batch_size, \n",
    "        verbose=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize scores\n",
    "def summarize_results(scores):\n",
    "    print(scores)\n",
    "    m, s = np.mean(scores), np.std(scores)\n",
    "    print('Accuracy: %.3f%% (+/-%.3f)' % (m, s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run an experiment\n",
    "def run_experiment(repeats=10):\n",
    "    \n",
    "    # load data\n",
    "    trainX, trainy, testX, testy = load_dataset()\n",
    "    \n",
    "    # repeat experiment\n",
    "    scores = list()\n",
    "    for r in range(repeats):\n",
    "        score = evaluate_model(trainX, trainy, testX, testy)\n",
    "        score = score * 100.0\n",
    "        print('>#%d: %.3f' % (r+1, score))\n",
    "        scores.append(score)\n",
    "    \n",
    "    # summarize results\n",
    "    summarize_results(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7352, 128, 9) (7352, 1)\n",
      "(2947, 128, 9) (2947, 1)\n",
      "(7352, 128, 9) (7352, 6) (2947, 128, 9) (2947, 6)\n",
      "Epoch 1/15\n",
      "7352/7352 [==============================] - 9s 1ms/step - loss: 1.2953 - acc: 0.4434\n",
      "Epoch 2/15\n",
      "7352/7352 [==============================] - 8s 1ms/step - loss: 0.8994 - acc: 0.6200\n",
      "Epoch 3/15\n",
      "7352/7352 [==============================] - 8s 1ms/step - loss: 0.6521 - acc: 0.7499\n",
      "Epoch 4/15\n",
      "7352/7352 [==============================] - 8s 1ms/step - loss: 0.6234 - acc: 0.7629\n",
      "Epoch 5/15\n",
      "7352/7352 [==============================] - 8s 1ms/step - loss: 0.3870 - acc: 0.8716\n",
      "Epoch 6/15\n",
      "7352/7352 [==============================] - 8s 1ms/step - loss: 0.2793 - acc: 0.9110\n",
      "Epoch 7/15\n",
      "7352/7352 [==============================] - 7s 1ms/step - loss: 0.2667 - acc: 0.9082\n",
      "Epoch 8/15\n",
      "7352/7352 [==============================] - 8s 1ms/step - loss: 0.2104 - acc: 0.9270\n",
      "Epoch 9/15\n",
      "7352/7352 [==============================] - 8s 1ms/step - loss: 0.1803 - acc: 0.9355\n",
      "Epoch 10/15\n",
      "7352/7352 [==============================] - 8s 1ms/step - loss: 0.1619 - acc: 0.9378\n",
      "Epoch 11/15\n",
      "7352/7352 [==============================] - 8s 1ms/step - loss: 0.1488 - acc: 0.9423\n",
      "Epoch 12/15\n",
      "7352/7352 [==============================] - 8s 1ms/step - loss: 0.3227 - acc: 0.8750\n",
      "Epoch 13/15\n",
      "7352/7352 [==============================] - 8s 1ms/step - loss: 0.2044 - acc: 0.9191\n",
      "Epoch 14/15\n",
      "7352/7352 [==============================] - 8s 1ms/step - loss: 0.1551 - acc: 0.9370\n",
      "Epoch 15/15\n",
      "7352/7352 [==============================] - 8s 1ms/step - loss: 0.1539 - acc: 0.9384\n",
      ">#1: 89.922\n",
      "Epoch 1/15\n",
      "7352/7352 [==============================] - 9s 1ms/step - loss: 1.1640 - acc: 0.5113\n",
      "Epoch 2/15\n",
      "7352/7352 [==============================] - 8s 1ms/step - loss: 0.8534 - acc: 0.6273\n",
      "Epoch 3/15\n",
      "7352/7352 [==============================] - 9s 1ms/step - loss: 0.6133 - acc: 0.7451\n",
      "Epoch 4/15\n",
      "7352/7352 [==============================] - 9s 1ms/step - loss: 0.4637 - acc: 0.8157\n",
      "Epoch 5/15\n",
      "7352/7352 [==============================] - 9s 1ms/step - loss: 0.4015 - acc: 0.8526\n",
      "Epoch 6/15\n",
      "7352/7352 [==============================] - 9s 1ms/step - loss: 0.2300 - acc: 0.9191\n",
      "Epoch 7/15\n",
      "7352/7352 [==============================] - 8s 1ms/step - loss: 0.2003 - acc: 0.9268\n",
      "Epoch 8/15\n",
      "7352/7352 [==============================] - 8s 1ms/step - loss: 0.2289 - acc: 0.9193\n",
      "Epoch 9/15\n",
      "7352/7352 [==============================] - 8s 1ms/step - loss: 0.1766 - acc: 0.9339\n",
      "Epoch 10/15\n",
      "7352/7352 [==============================] - 8s 1ms/step - loss: 0.1451 - acc: 0.9412\n",
      "Epoch 11/15\n",
      "7352/7352 [==============================] - 9s 1ms/step - loss: 0.1544 - acc: 0.9406\n",
      "Epoch 12/15\n",
      "7352/7352 [==============================] - 9s 1ms/step - loss: 0.1347 - acc: 0.9478\n",
      "Epoch 13/15\n",
      "7352/7352 [==============================] - 8s 1ms/step - loss: 0.1798 - acc: 0.9320\n",
      "Epoch 14/15\n",
      "7352/7352 [==============================] - 8s 1ms/step - loss: 0.1551 - acc: 0.9402\n",
      "Epoch 15/15\n",
      "7352/7352 [==============================] - 8s 1ms/step - loss: 0.1486 - acc: 0.9430\n",
      ">#2: 90.567\n",
      "[89.92195453003053, 90.56667797760434]\n",
      "Accuracy: 90.244% (+/-0.322)\n"
     ]
    }
   ],
   "source": [
    "# !!! THIS TAKES A WHILE !!!\n",
    "# run the experiment\n",
    "run_experiment(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
